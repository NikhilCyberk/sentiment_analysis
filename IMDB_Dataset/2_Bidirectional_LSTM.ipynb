{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_text_preprocessing(content):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # More comprehensive preprocessing\n",
    "    lemmatized_content = re.sub(r'http\\S+|www\\S+|https\\S+', '', content, flags=re.MULTILINE)\n",
    "    lemmatized_content = re.sub(r'@\\w+', '', lemmatized_content)  # Remove mentions\n",
    "    lemmatized_content = re.sub(r'\\#', '', lemmatized_content)  # Remove hashtag symbols\n",
    "    lemmatized_content = re.sub(r'[^\\w\\s]', '', lemmatized_content)  # Remove punctuation\n",
    "    lemmatized_content = lemmatized_content.lower()\n",
    "\n",
    "    # More advanced tokenization\n",
    "    words = word_tokenize(lemmatized_content)\n",
    "\n",
    "    # Enhanced stopwords removal and lemmatization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stopwords = {'rt', 'via', 'amp', 'ur', 'u'}\n",
    "    stop_words.update(custom_stopwords)\n",
    "    \n",
    "    processed_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words \n",
    "        if word not in stop_words and len(word) > 1\n",
    "    ]\n",
    "    lemmatized_content = ' '.join(processed_words)\n",
    "    return lemmatized_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    # column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "    data = pd.read_csv(\"IMDB_Dataset.csv\", encoding='ISO-8859-1')\n",
    "    # data['target'] = data['target'].replace(4, 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_deep_learning_model(vocab_size, max_length):\n",
    "    \"\"\"Create an advanced deep learning model\"\"\"\n",
    "    model = Sequential([\n",
    "        # Enhanced embedding layer\n",
    "        Embedding(vocab_size, 128, input_length=max_length, trainable=True),\n",
    "        \n",
    "        # Spatial dropout to prevent overfitting\n",
    "        SpatialDropout1D(0.3),\n",
    "        \n",
    "        # Bidirectional LSTM for capturing context in both directions\n",
    "        Bidirectional(LSTM(\n",
    "            128, \n",
    "            dropout=0.2, \n",
    "            recurrent_dropout=0.2, \n",
    "            return_sequences=True\n",
    "        )),\n",
    "        \n",
    "        # Another LSTM layer\n",
    "        Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        \n",
    "        # More dense layers with dropout\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Advanced compilation with custom learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_advanced_data(texts, max_words=10000, max_length=150):\n",
    "    \"\"\"Advanced data preparation with more tokens and longer sequences\"\"\"\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(\n",
    "        sequences, \n",
    "        maxlen=max_length, \n",
    "        padding='post', \n",
    "        truncating='post'\n",
    "    )\n",
    "    \n",
    "    return padded_sequences, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_advanced_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Advanced training with multiple optimization techniques\"\"\"\n",
    "    # Prepare sequences\n",
    "    max_words = 10000\n",
    "    max_length = 150\n",
    "    \n",
    "    X_train_seq, tokenizer = prepare_advanced_data(X_train, max_words, max_length)\n",
    "    X_test_seq, _ = prepare_advanced_data(X_test, max_words, max_length)\n",
    "    \n",
    "    # Calculate class weights to handle imbalanced data\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        'balanced', \n",
    "        classes=np.unique(y_train), \n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Create model\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    model = create_advanced_deep_learning_model(vocab_size, max_length)\n",
    "    \n",
    "    # Advanced training callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=5, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.2, \n",
    "        patience=3, \n",
    "        min_lr=0.00001\n",
    "    )\n",
    "    \n",
    "    # Train with advanced techniques\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train, \n",
    "        epochs=20, \n",
    "        batch_size=64, \n",
    "        validation_split=0.2,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    train_pred = (model.predict(X_train_seq) > 0.5).astype(int).flatten()\n",
    "    test_pred = (model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "    \n",
    "    results = {\n",
    "        'Enhanced Deep Learning Model': {\n",
    "            'train_accuracy': accuracy_score(y_train, train_pred),\n",
    "            'test_accuracy': accuracy_score(y_test, test_pred),\n",
    "            'classification_report': classification_report(y_test, test_pred)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    }
   ],
   "source": [
    " # Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "data = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  target\n",
       "0  One of the other reviewers has mentioned that ...  positive       1\n",
       "1  A wonderful little production. <br /><br />The...  positive       1\n",
       "2  I thought this was a wonderful way to spend ti...  positive       1\n",
       "3  Basically there's a family where a little boy ...  negative       0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_12020\\3214355603.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['target'] = data['sentiment'].replace('positive', 1).replace('negative', 0)\n"
     ]
    }
   ],
   "source": [
    "data['target'] = data['sentiment'].replace('positive', 1).replace('negative', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "target       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    25000\n",
       "0    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the distribution of target col\n",
    "\n",
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "\n",
    "# data = data.sample(n=30000, random_state=42)\n",
    "\n",
    "data['lemmatized_content'] = data['review'].apply(advanced_text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = data['lemmatized_content'].values\n",
    "y = data['target'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training advanced deep learning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 421ms/step - accuracy: 0.6380 - loss: 0.6047 - val_accuracy: 0.5677 - val_loss: 0.6415 - learning_rate: 5.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 420ms/step - accuracy: 0.7318 - loss: 0.5114 - val_accuracy: 0.7559 - val_loss: 0.5181 - learning_rate: 5.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 431ms/step - accuracy: 0.8551 - loss: 0.3613 - val_accuracy: 0.8650 - val_loss: 0.3372 - learning_rate: 5.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 433ms/step - accuracy: 0.8950 - loss: 0.2801 - val_accuracy: 0.8775 - val_loss: 0.3085 - learning_rate: 5.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 435ms/step - accuracy: 0.9130 - loss: 0.2367 - val_accuracy: 0.8802 - val_loss: 0.3185 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 427ms/step - accuracy: 0.9326 - loss: 0.1914 - val_accuracy: 0.8773 - val_loss: 0.3111 - learning_rate: 5.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 429ms/step - accuracy: 0.9410 - loss: 0.1716 - val_accuracy: 0.8773 - val_loss: 0.3538 - learning_rate: 5.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 430ms/step - accuracy: 0.9605 - loss: 0.1246 - val_accuracy: 0.8796 - val_loss: 0.3788 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 429ms/step - accuracy: 0.9640 - loss: 0.1114 - val_accuracy: 0.8792 - val_loss: 0.4102 - learning_rate: 1.0000e-04\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step\n"
     ]
    }
   ],
   "source": [
    " # Train advanced model\n",
    "print(\"Training advanced deep learning model...\")\n",
    "\n",
    "dl_results, dl_model, tokenizer = train_advanced_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/20\n",
    "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 421ms/step - accuracy: 0.6380 - loss: 0.6047 - val_accuracy: 0.5677 - val_loss: 0.6415 - learning_rate: 5.0000e-04\n",
    "Epoch 2/20\n",
    "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 420ms/step - accuracy: 0.7318 - loss: 0.5114 - val_accuracy: 0.7559 - val_loss: 0.5181 - learning_rate: 5.0000e-04\n",
    "Epoch 3/20\n",
    "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 431ms/step - accuracy: 0.8551 - loss: 0.3613 - val_accuracy: 0.8650 - val_loss: 0.3372 - learning_rate: 5.0000e-04\n",
    "Epoch 4/20\n",
    "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 433ms/step - accuracy: 0.8950 - loss: 0.2801 - val_accuracy: 0.8775 - val_loss: 0.3085 - learning_rate: 5.0000e-04\n",
    "Epoch 5/20\n",
    "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 435ms/step - accuracy: 0.9130 - loss: 0.2367 - val_accuracy: 0.8802 - val_loss: 0.3185 - learning_rate: 5.0000e-04\n",
    "Epoch 6/20\n",
    "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 427ms/step - accuracy: 0.9326 - loss: 0.1914 - val_accuracy: 0.8773 - val_loss: 0.3111 - learning_rate: 5.0000e-04\n",
    "Epoch 7/20\n",
    "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 429ms/step - accuracy: 0.9410 - loss: 0.1716 - val_accuracy: 0.8773 - val_loss: 0.3538 - learning_rate: 5.0000e-04\n",
    "Epoch 8/20\n",
    "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 430ms/step - accuracy: 0.9605 - loss: 0.1246 - val_accuracy: 0.8796 - val_loss: 0.3788 - learning_rate: 1.0000e-04\n",
    "Epoch 9/20\n",
    "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 429ms/step - accuracy: 0.9640 - loss: 0.1114 - val_accuracy: 0.8792 - val_loss: 0.4102 - learning_rate: 1.0000e-04\n",
    "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step\n",
    "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Deep Learning Model Results:\n",
      "Training Accuracy: 0.9209\n",
      "Test Accuracy: 0.5177\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.58      0.55      5000\n",
      "           1       0.52      0.45      0.48      5000\n",
      "\n",
      "    accuracy                           0.52     10000\n",
      "   macro avg       0.52      0.52      0.52     10000\n",
      "weighted avg       0.52      0.52      0.52     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  # Print results\n",
    "    \n",
    "for model_name, metrics in dl_results.items():\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Training Accuracy: {metrics['train_accuracy']:.4f}\")\n",
    "    print(f\"Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(metrics['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Enhanced Deep Learning Model Results:\n",
    "Training Accuracy: 0.9209\n",
    "Test Accuracy: 0.5177\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.52      0.58      0.55      5000\n",
    "           1       0.52      0.45      0.48      5000\n",
    "\n",
    "    accuracy                           0.52     10000\n",
    "   macro avg       0.52      0.52      0.52     10000\n",
    "weighted avg       0.52      0.52      0.52     10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2931570926.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 25\u001b[1;36m\u001b[0m\n\u001b[1;33m    | 1st LSTM | Bidirectional, 128 units |\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Sentiment Analysis Performance Report\n",
    "\n",
    "## Overall Performance Metrics\n",
    "\n",
    "| Category | Value |\n",
    "|----------|-------|\n",
    "| Training Accuracy | 92.09% |\n",
    "| Test Accuracy | 51.77% |\n",
    "| Model Type | Bidirectional LSTM |\n",
    "| Preprocessing | Advanced Lemmatization |\n",
    "\n",
    "## Detailed Performance Metrics\n",
    "\n",
    "| Metric | Negative (Class 0) | Positive (Class 1) | Macro Average |\n",
    "|--------|-------------------|-------------------|--------------|\n",
    "| Precision | 0.52 | 0.52 | 0.52 |\n",
    "| Recall | 0.58 | 0.45 | 0.52 |\n",
    "| F1-Score | 0.55 | 0.48 | 0.52 |\n",
    "\n",
    "## Model Architecture Details\n",
    "\n",
    "| Layer | Configuration |\n",
    "|-------|---------------|\n",
    "| Embedding | 128 dimensions |\n",
    "| 1st LSTM | Bidirectional, 128 units |\n",
    "| 2nd LSTM | Bidirectional, 64 units |\n",
    "| Dense Layers | 64 and 32 units, ReLU activation |\n",
    "| Output Layer | Sigmoid activation |\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Optimizer | Adam |\n",
    "| Learning Rate | 0.0005 |\n",
    "| Batch Size | 64 |\n",
    "| Epochs | 20 |\n",
    "| Early Stopping | Patience of 5 |\n",
    "| Learning Rate Reduction | Patience of 3 |\n",
    "\n",
    "## Preprocessing Techniques\n",
    "\n",
    "| Technique | Description |\n",
    "|-----------|-------------|\n",
    "| Tokenization | Word-level |\n",
    "| Text Cleaning | URL removal, mention removal |\n",
    "| Lemmatization | WordNet Lemmatizer |\n",
    "| Stopword Removal | Enhanced custom stopwords |\n",
    "\n",
    "## Key Observations\n",
    "\n",
    "| Aspect | Insight |\n",
    "|--------|---------|\n",
    "| Overfitting | High training accuracy, low test accuracy |\n",
    "| Model Complexity | Advanced architecture with multiple techniques |\n",
    "| Potential Improvements | Feature engineering, regularization |\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "| Area | Suggested Action |\n",
    "|------|-----------------|\n",
    "| Overfitting | Increase regularization |\n",
    "| Data Preprocessing | Experiment with feature extraction |\n",
    "| Model Tuning | Adjust hyperparameters |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
