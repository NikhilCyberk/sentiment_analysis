{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Sentiment Analysis Model Comparison\n",
    "\n",
    "| Metric | BERT Transformer | LSTM Deep Learning | RoBERTa Transformer | DistilBERT Transformer | Logistic Regression | Random Forest | LSTM Deep Learning |\n",
    "|--------|-----------------|--------------------|--------------------|----------------------|---------------------|---------------|---------------------|\n",
    "| **Base Model** | BERT-base-uncased | Bidirectional LSTM | CardiffNLP Twitter RoBERTa | DistilBERT | TF-IDF Vectorization | TF-IDF Vectorization | Custom LSTM |\n",
    "| **Dataset Size** | 30,000 | - | 50,000 | 4,000 | 30,000 | 30,000 | 100,000 |\n",
    "| **Accuracy** | 0.7775 | 0.52 | 0.85 | 0.80 | 0.7438 | 0.7277 | 0.5003 |\n",
    "| **Precision (Negative)** | 0.79 | 0.53 | 0.82 | 0.76 | 0.75 | 0.73 | 0.00 |\n",
    "| **Precision (Positive)** | 0.77 | 0.52 | 0.87 | 0.86 | 0.74 | 0.73 | 0.50 |\n",
    "| **Recall (Negative)** | 0.76 | 0.47 | 0.88 | 0.87 | 0.73 | 0.72 | 0.00 |\n",
    "| **Recall (Positive)** | 0.80 | 0.58 | 0.81 | 0.73 | 0.76 | 0.73 | 1.00 |\n",
    "| **F1-Score (Negative)** | 0.77 | 0.49 | 0.85 | 0.82 | 0.74 | 0.73 | 0.00 |\n",
    "| **F1-Score (Positive)** | 0.78 | 0.55 | 0.84 | 0.79 | 0.75 | 0.73 | 0.67 |\n",
    "| **Training Accuracy** | 0.8764 | 0.8078 | - | - | 0.8094 | 0.9840 | 0.5003 |\n",
    "| **Epochs** | 3 | 7 | 3 | 3 | - | - | 10 |\n",
    "| **Batch Size** | 32 | 64 | 16 | 16 | - | - | 32 |\n",
    "| **Key Observations** | Moderate overfitting, stable validation accuracy | Significant performance drop between training and testing | Balanced performance across classes | Lightweight model with consistent performance | Less overfitting, better generalization | Significant overfitting | Failed to learn meaningful representations |\n",
    "\n",
    "## Key Insights\n",
    "- Transformer models (BERT, RoBERTa, DistilBERT) generally outperform traditional machine learning and simple deep learning models\n",
    "- Performance varies significantly across different architectures and preprocessing techniques\n",
    "- Most models show some level of overfitting\n",
    "- Accuracy ranges from 0.52 to 0.85 across different approaches\n",
    "\n",
    "## Recommended Next Steps\n",
    "1. Experiment with advanced transformer models\n",
    "2. Implement robust preprocessing techniques\n",
    "3. Use data augmentation\n",
    "4. Apply regularization methods\n",
    "5. Collect more diverse training data\n",
    "\n",
    "**Last Updated**: November 28, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
