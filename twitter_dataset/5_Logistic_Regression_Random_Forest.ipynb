{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess the Twitter dataset\"\"\"\n",
    "    column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "    data = pd.read_csv(\"twitter_dataset.csv\", names=column_names, encoding='ISO-8859-1')\n",
    "    data['target'] = data['target'].replace(4, 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample data 10,000 rows randomly\n",
    "data = load_and_preprocess_data()\n",
    "data = data.sample(n=30000, random_state=42)\n",
    "data.head()\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"\"\"Main function to run the improved sentiment analysis\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "# print(\"Loading and preprocessing data...\")\n",
    "\n",
    "# data = load_and_preprocess_data()\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541200</th>\n",
       "      <td>0</td>\n",
       "      <td>2200003196</td>\n",
       "      <td>Tue Jun 16 18:18:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LaLaLindsey0609</td>\n",
       "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0</td>\n",
       "      <td>1467998485</td>\n",
       "      <td>Mon Apr 06 23:11:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sexygrneyes</td>\n",
       "      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766711</th>\n",
       "      <td>0</td>\n",
       "      <td>2300048954</td>\n",
       "      <td>Tue Jun 23 13:40:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammydearr</td>\n",
       "      <td>@TiannaChaos i know  just family drama. its la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285055</th>\n",
       "      <td>0</td>\n",
       "      <td>1993474027</td>\n",
       "      <td>Mon Jun 01 10:26:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Lamb_Leanne</td>\n",
       "      <td>School email won't open  and I have geography ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705995</th>\n",
       "      <td>0</td>\n",
       "      <td>2256550904</td>\n",
       "      <td>Sat Jun 20 12:56:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>yogicerdito</td>\n",
       "      <td>upper airways problem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target         ids                          date      flag  \\\n",
       "541200       0  2200003196  Tue Jun 16 18:18:12 PDT 2009  NO_QUERY   \n",
       "750          0  1467998485  Mon Apr 06 23:11:14 PDT 2009  NO_QUERY   \n",
       "766711       0  2300048954  Tue Jun 23 13:40:11 PDT 2009  NO_QUERY   \n",
       "285055       0  1993474027  Mon Jun 01 10:26:07 PDT 2009  NO_QUERY   \n",
       "705995       0  2256550904  Sat Jun 20 12:56:51 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \n",
       "541200  LaLaLindsey0609             @chrishasboobs AHHH I HOPE YOUR OK!!!   \n",
       "750         sexygrneyes  @misstoriblack cool , i have no tweet apps  fo...  \n",
       "766711       sammydearr  @TiannaChaos i know  just family drama. its la...  \n",
       "285055      Lamb_Leanne  School email won't open  and I have geography ...  \n",
       "705995      yogicerdito                             upper airways problem   "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 6)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "ids       0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting the num of missing values in the dataset\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    15001\n",
       "0    14999\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the distribution of target col\n",
    "\n",
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_text_preprocessing(content):\n",
    "\n",
    "    # Initialize lemmatizer\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Convert to lowercase and remove special characters\n",
    "    lemmatized_content = re.sub(r'http\\S+|www\\S+|https\\S+', '', content, flags=re.MULTILINE)  # Remove URLs\n",
    "    lemmatized_content = re.sub(r'@\\w+|\\#', '', lemmatized_content)  # Remove mentions and hashtag symbols\n",
    "    lemmatized_content = re.sub(r'[^a-zA-Z\\s]', '', lemmatized_content)  # Remove numbers and punctuation\n",
    "    lemmatized_content = lemmatized_content.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    words = word_tokenize(lemmatized_content)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Add custom stopwords that might not be useful for sentiment\n",
    "    custom_stopwords = {'rt', 'via'}\n",
    "    stop_words.update(custom_stopwords)\n",
    "    processed_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 2\n",
    "    ]\n",
    "    lemmatized_content = ' '.join(processed_words)\n",
    "    return lemmatized_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541200               @chrishasboobs AHHH I HOPE YOUR OK!!! \n",
       "750       @misstoriblack cool , i have no tweet apps  fo...\n",
       "766711    @TiannaChaos i know  just family drama. its la...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data['text'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text preprocessing...\n"
     ]
    }
   ],
   "source": [
    " # Apply advanced text preprocessing\n",
    "    \n",
    "print(\"Applying text preprocessing...\")\n",
    "    \n",
    "data['lemmatized_content'] = data['text'].apply(advanced_text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541200                                            ahhh hope\n",
       "750                                    cool tweet apps razr\n",
       "766711    know family drama lamehey next time hang kim g...\n",
       "Name: lemmatized_content, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemmatized_content'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_tfidf():\n",
    "    \"\"\"Create an advanced TF-IDF vectorizer\"\"\"\n",
    "    return TfidfVectorizer(\n",
    "        min_df=5,  # Minimum document frequency\n",
    "        max_df=0.8,  # Maximum document frequency\n",
    "        ngram_range=(1, 2),  # Include both unigrams and bigrams\n",
    "        sublinear_tf=True,  # Apply sublinear scaling to term frequencies\n",
    "        strip_accents='unicode',\n",
    "        token_pattern=r'\\b\\w+\\b'  # Only match word characters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train and evaluate multiple models using GridSearchCV\"\"\"\n",
    "    \n",
    "    # Create pipelines for different models\n",
    "    lr_pipeline = Pipeline([\n",
    "        ('tfidf', create_advanced_tfidf()),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    rf_pipeline = Pipeline([\n",
    "        ('tfidf', create_advanced_tfidf()),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grids for GridSearchCV\n",
    "    lr_param_grid = {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__class_weight': ['balanced', None],\n",
    "        'classifier__max_iter': [1000]\n",
    "    }\n",
    "    \n",
    "    rf_param_grid = {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [10, 20, None],\n",
    "        'classifier__class_weight': ['balanced', 'balanced_subsample']\n",
    "    }\n",
    "    \n",
    "    # Perform GridSearchCV for both models\n",
    "    lr_grid = GridSearchCV(lr_pipeline, lr_param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "    rf_grid = GridSearchCV(rf_pipeline, rf_param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "    \n",
    "    # Train both models\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    lr_grid.fit(X_train, y_train)\n",
    "    print(\"Training Random Forest...\")\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best models\n",
    "    best_lr = lr_grid.best_estimator_\n",
    "    best_rf = rf_grid.best_estimator_\n",
    "    \n",
    "    # Evaluate models\n",
    "    models = {\n",
    "        'Logistic Regression': best_lr,\n",
    "        'Random Forest': best_rf\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        train_pred = model.predict(X_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "        \n",
    "        results[name] = {\n",
    "            'train_accuracy': accuracy_score(y_train, train_pred),\n",
    "            'test_accuracy': accuracy_score(y_test, test_pred),\n",
    "            'classification_report': classification_report(y_test, test_pred)\n",
    "        }\n",
    "    \n",
    "    return results, best_lr if lr_grid.best_score_ > rf_grid.best_score_ else best_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541200</th>\n",
       "      <td>0</td>\n",
       "      <td>2200003196</td>\n",
       "      <td>Tue Jun 16 18:18:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LaLaLindsey0609</td>\n",
       "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
       "      <td>ahhh hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0</td>\n",
       "      <td>1467998485</td>\n",
       "      <td>Mon Apr 06 23:11:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sexygrneyes</td>\n",
       "      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n",
       "      <td>cool tweet apps razr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766711</th>\n",
       "      <td>0</td>\n",
       "      <td>2300048954</td>\n",
       "      <td>Tue Jun 23 13:40:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammydearr</td>\n",
       "      <td>@TiannaChaos i know  just family drama. its la...</td>\n",
       "      <td>know family drama lamehey next time hang kim g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285055</th>\n",
       "      <td>0</td>\n",
       "      <td>1993474027</td>\n",
       "      <td>Mon Jun 01 10:26:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Lamb_Leanne</td>\n",
       "      <td>School email won't open  and I have geography ...</td>\n",
       "      <td>school email wont open geography stuff revise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705995</th>\n",
       "      <td>0</td>\n",
       "      <td>2256550904</td>\n",
       "      <td>Sat Jun 20 12:56:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>yogicerdito</td>\n",
       "      <td>upper airways problem</td>\n",
       "      <td>upper airway problem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target         ids                          date      flag  \\\n",
       "541200       0  2200003196  Tue Jun 16 18:18:12 PDT 2009  NO_QUERY   \n",
       "750          0  1467998485  Mon Apr 06 23:11:14 PDT 2009  NO_QUERY   \n",
       "766711       0  2300048954  Tue Jun 23 13:40:11 PDT 2009  NO_QUERY   \n",
       "285055       0  1993474027  Mon Jun 01 10:26:07 PDT 2009  NO_QUERY   \n",
       "705995       0  2256550904  Sat Jun 20 12:56:51 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \\\n",
       "541200  LaLaLindsey0609             @chrishasboobs AHHH I HOPE YOUR OK!!!    \n",
       "750         sexygrneyes  @misstoriblack cool , i have no tweet apps  fo...   \n",
       "766711       sammydearr  @TiannaChaos i know  just family drama. its la...   \n",
       "285055      Lamb_Leanne  School email won't open  and I have geography ...   \n",
       "705995      yogicerdito                             upper airways problem    \n",
       "\n",
       "                                       lemmatized_content  \n",
       "541200                                          ahhh hope  \n",
       "750                                  cool tweet apps razr  \n",
       "766711  know family drama lamehey next time hang kim g...  \n",
       "285055  school email wont open geography stuff revise ...  \n",
       "705995                               upper airway problem  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemmatized_content'].values\n",
    "y = data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = data['lemmatized_content'].values\n",
    "y = data['target'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models...\n",
      "Training Logistic Regression...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Training Random Forest...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Logistic Regression Results:\n",
      "Training Accuracy: 0.8094\n",
      "Test Accuracy: 0.7438\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.73      0.74      3000\n",
      "           1       0.74      0.76      0.75      3000\n",
      "\n",
      "    accuracy                           0.74      6000\n",
      "   macro avg       0.74      0.74      0.74      6000\n",
      "weighted avg       0.74      0.74      0.74      6000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Random Forest Results:\n",
      "Training Accuracy: 0.9840\n",
      "Test Accuracy: 0.7277\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73      3000\n",
      "           1       0.73      0.73      0.73      3000\n",
      "\n",
      "    accuracy                           0.73      6000\n",
      "   macro avg       0.73      0.73      0.73      6000\n",
      "weighted avg       0.73      0.73      0.73      6000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Train and evaluate models\n",
    "print(\"Training models...\")\n",
    "results, best_model = train_model(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Training Accuracy: {metrics['train_accuracy']:.4f}\")\n",
    "    print(f\"Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(metrics['classification_report'])    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'twitter_model_lemit.pkl'\n",
    "pickle.dump(best_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved model\n",
    "\n",
    "loaded_model = pickle.load(open('twitter_model_lemit.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(y_test[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Negative tweet\n"
     ]
    }
   ],
   "source": [
    "prediction = (loaded_model.predict([X_new]))\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "if (prediction[0]==0):\n",
    "    print('Negative tweet')\n",
    "else:\n",
    "    print('Positive tweet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Model Comparison Report\n",
    "\n",
    "## Overview\n",
    "This report presents a comparative analysis of Logistic Regression and Random Forest models for Twitter sentiment classification using advanced text preprocessing and TF-IDF vectorization.\n",
    "\n",
    "## 1. Model Performance Comparison\n",
    "\n",
    "| Metric | Logistic Regression | Random Forest |\n",
    "|--------|---------------------|---------------|\n",
    "| **Training Accuracy** | 0.8094 | 0.9840 |\n",
    "| **Test Accuracy** | 0.7438 | 0.7277 |\n",
    "| **Precision (Negative)** | 0.75 | 0.73 |\n",
    "| **Precision (Positive)** | 0.74 | 0.73 |\n",
    "| **Recall (Negative)** | 0.73 | 0.72 |\n",
    "| **Recall (Positive)** | 0.76 | 0.73 |\n",
    "| **F1-Score (Negative)** | 0.74 | 0.73 |\n",
    "| **F1-Score (Positive)** | 0.75 | 0.73 |\n",
    "\n",
    "## 2. Detailed Performance Analysis\n",
    "\n",
    "### 2.1 Accuracy\n",
    "- Both models demonstrate comparable performance on the test set, with Logistic Regression slightly outperforming Random Forest.\n",
    "- Logistic Regression achieved a test accuracy of 74.38%\n",
    "- Random Forest achieved a test accuracy of 72.77%\n",
    "\n",
    "### 2.2 Bias and Variance\n",
    "- Logistic Regression shows less overfitting:\n",
    "  - Training Accuracy: 80.94%\n",
    "  - Test Accuracy: 74.38%\n",
    "  - Variance Gap: ~6.56%\n",
    "\n",
    "- Random Forest shows significant overfitting:\n",
    "  - Training Accuracy: 98.40%\n",
    "  - Test Accuracy: 72.77%\n",
    "  - Variance Gap: ~25.63%\n",
    "\n",
    "## 3. Model Complexity Analysis\n",
    "\n",
    "### 3.1 Logistic Regression\n",
    "- **Pros**:\n",
    "  - Simpler model\n",
    "  - Faster training and prediction\n",
    "  - Less prone to overfitting\n",
    "  - Better generalization\n",
    "- **Cons**:\n",
    "  - Assumes linear relationship\n",
    "  - Less complex feature interactions\n",
    "\n",
    "### 3.2 Random Forest\n",
    "- **Pros**:\n",
    "  - Handles non-linear relationships\n",
    "  - Robust to outliers\n",
    "  - Can capture complex feature interactions\n",
    "- **Cons**:\n",
    "  - Prone to overfitting\n",
    "  - Computationally more expensive\n",
    "  - Less interpretable\n",
    "\n",
    "## 4. Key Preprocessing Techniques\n",
    "\n",
    "### Text Preprocessing\n",
    "- Lemmatization\n",
    "- URL and special character removal\n",
    "- Stopword elimination\n",
    "- Custom stopword addition\n",
    "\n",
    "### Feature Extraction\n",
    "- TF-IDF Vectorization\n",
    "  - Unigrams and bigrams\n",
    "  - Minimum document frequency: 5\n",
    "  - Maximum document frequency: 0.8\n",
    "  - Sublinear scaling of term frequencies\n",
    "\n",
    "## 5. Recommendations\n",
    "\n",
    "1. **Model Selection**: \n",
    "   - Logistic Regression is recommended due to:\n",
    "     - Better test accuracy\n",
    "     - Lower overfitting\n",
    "     - Faster computation\n",
    "\n",
    "2. **Future Improvements**:\n",
    "   - Experiment with more advanced models (e.g., BERT, RoBERTa)\n",
    "   - Collect more diverse training data\n",
    "   - Implement cross-validation with more folds\n",
    "   - Explore ensemble methods\n",
    "\n",
    "## 6. Conclusion\n",
    "The Logistic Regression model provides a more robust and generalizable solution for Twitter sentiment analysis in this context, balancing performance and computational efficiency.\n",
    "\n",
    "## Appendix: Preprocessing Code Snippet\n",
    "\n",
    "```python\n",
    "def advanced_text_preprocessing(content):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Remove URLs and special characters\n",
    "    content = re.sub(r'http\\S+|www\\S+|https\\S+', '', content, flags=re.MULTILINE)\n",
    "    content = re.sub(r'@\\w+|\\#', '', content)\n",
    "    content = re.sub(r'[^a-zA-Z\\s]', '', content)\n",
    "    content = content.lower()\n",
    "\n",
    "    # Tokenization and lemmatization\n",
    "    words = word_tokenize(content)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stopwords = {'rt', 'via'}\n",
    "    stop_words.update(custom_stopwords)\n",
    "    \n",
    "    processed_words = [\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in words \n",
    "        if word not in stop_words and len(word) > 2\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(processed_words)\n",
    "```\n",
    "\n",
    "**Date of Analysis**: November 27, 2024\n",
    "**Dataset**: Twitter Sentiment Dataset\n",
    "**Sample Size**: 30,000 tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
