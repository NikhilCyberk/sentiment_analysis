{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def verify_cuda_setup():\n",
    "#     \"\"\"\n",
    "#     Verify CUDA setup and provide detailed information\n",
    "#     \"\"\"\n",
    "#     print(\"\\n=== CUDA Setup Verification ===\")\n",
    "    \n",
    "#     # Check PyTorch installation\n",
    "#     print(f\"PyTorch Version: {torch.__version__}\")\n",
    "#     print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "#     if torch.cuda.is_available():\n",
    "#         # Get CUDA version\n",
    "#         cuda_version = torch.version.cuda\n",
    "#         print(f\"CUDA Version: {cuda_version}\")\n",
    "        \n",
    "#         # GPU Device information\n",
    "#         current_device = torch.cuda.current_device()\n",
    "#         print(f\"Current CUDA device index: {current_device}\")\n",
    "#         print(f\"GPU Device: {torch.cuda.get_device_name(current_device)}\")\n",
    "        \n",
    "#         # Memory information\n",
    "#         print(\"\\nGPU Memory Information:\")\n",
    "#         print(f\"Total GPU Memory: {torch.cuda.get_device_properties(current_device).total_memory / 1024**3:.2f} GB\")\n",
    "#         print(f\"Memory Allocated: {torch.cuda.memory_allocated(current_device) / 1024**3:.2f} GB\")\n",
    "#         print(f\"Memory Cached: {torch.cuda.memory_reserved(current_device) / 1024**3:.2f} GB\")\n",
    "        \n",
    "#         # CUDA architecture\n",
    "#         print(f\"\\nCUDA Architecture:\")\n",
    "#         print(f\"Device Capability: {torch.cuda.get_device_capability(current_device)}\")\n",
    "        \n",
    "#         # Test CUDA computation\n",
    "#         print(\"\\nTesting CUDA computation...\")\n",
    "#         try:\n",
    "#             test_tensor = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n",
    "#             result = test_tensor.sum()\n",
    "#             print(\"CUDA computation test: Successful\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"CUDA computation test failed: {e}\")\n",
    "            \n",
    "#         return torch.device('cuda')\n",
    "#     else:\n",
    "#         print(\"\\nWARNING: CUDA is not available. Check your PyTorch installation and NVIDIA drivers.\")\n",
    "#         print(\"\\nDebug Information:\")\n",
    "#         print(f\"PyTorch CUDA arch list: {os.environ.get('TORCH_CUDA_ARCH_LIST', 'Not Set')}\")\n",
    "#         print(f\"CUDA_HOME: {os.environ.get('CUDA_HOME', 'Not Set')}\")\n",
    "#         print(f\"CUDA_PATH: {os.environ.get('CUDA_PATH', 'Not Set')}\")\n",
    "#         return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CUDA Setup Verification ===\n",
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA Version: 12.1\n",
      "Current CUDA device index: 0\n",
      "GPU Device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "\n",
      "GPU Memory Information:\n",
      "Total GPU Memory: 4.00 GB\n",
      "Memory Allocated: 0.00 GB\n",
      "Memory Cached: 0.00 GB\n",
      "\n",
      "CUDA Architecture:\n",
      "Device Capability: (8, 6)\n",
      "\n",
      "Testing CUDA computation...\n",
      "CUDA computation test: Successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_cuda_setup():\n",
    "    \"\"\"\n",
    "    Verify CUDA setup and provide detailed information\n",
    "    \"\"\"\n",
    "    print(\"\\n=== CUDA Setup Verification ===\")\n",
    "    \n",
    "    # Check PyTorch installation\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        # Get CUDA version\n",
    "        cuda_version = torch.version.cuda\n",
    "        print(f\"CUDA Version: {cuda_version}\")\n",
    "        \n",
    "        # GPU Device information\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(f\"Current CUDA device index: {current_device}\")\n",
    "        print(f\"GPU Device: {torch.cuda.get_device_name(current_device)}\")\n",
    "        \n",
    "        # Memory information\n",
    "        print(\"\\nGPU Memory Information:\")\n",
    "        print(f\"Total GPU Memory: {torch.cuda.get_device_properties(current_device).total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated(current_device) / 1024**3:.2f} GB\")\n",
    "        print(f\"Memory Cached: {torch.cuda.memory_reserved(current_device) / 1024**3:.2f} GB\")\n",
    "        \n",
    "        # CUDA architecture\n",
    "        print(f\"\\nCUDA Architecture:\")\n",
    "        print(f\"Device Capability: {torch.cuda.get_device_capability(current_device)}\")\n",
    "        \n",
    "        # Test CUDA computation\n",
    "        print(\"\\nTesting CUDA computation...\")\n",
    "        try:\n",
    "            test_tensor = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n",
    "            result = test_tensor.sum()\n",
    "            print(\"CUDA computation test: Successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"CUDA computation test failed: {e}\")\n",
    "            \n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        print(\"\\nWARNING: CUDA is not available. Check your PyTorch installation and NVIDIA drivers.\")\n",
    "        print(\"\\nDebug Information:\")\n",
    "        print(f\"PyTorch CUDA arch list: {os.environ.get('TORCH_CUDA_ARCH_LIST', 'Not Set')}\")\n",
    "        print(f\"CUDA_HOME: {os.environ.get('CUDA_HOME', 'Not Set')}\")\n",
    "        print(f\"CUDA_PATH: {os.environ.get('CUDA_PATH', 'Not Set')}\")\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CUDA Setup Verification ===\n",
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA Version: 12.1\n",
      "Current CUDA device index: 0\n",
      "GPU Device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "\n",
      "GPU Memory Information:\n",
      "Total GPU Memory: 4.00 GB\n",
      "Memory Allocated: 0.00 GB\n",
      "Memory Cached: 0.00 GB\n",
      "\n",
      "CUDA Architecture:\n",
      "Device Capability: (8, 6)\n",
      "\n",
      "Testing CUDA computation...\n",
      "CUDA computation test: Successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_cuda_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess the Twitter dataset\"\"\"\n",
    "    column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "    data = pd.read_csv(\"twitter_dataset.csv\", names=column_names, encoding='ISO-8859-1')\n",
    "    data['target'] = data['target'].replace(4, 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_and_preprocess_data()\n",
    "data = data.sample(n=30000, random_state=42)\n",
    "data.head()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541200</th>\n",
       "      <td>0</td>\n",
       "      <td>2200003196</td>\n",
       "      <td>Tue Jun 16 18:18:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LaLaLindsey0609</td>\n",
       "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0</td>\n",
       "      <td>1467998485</td>\n",
       "      <td>Mon Apr 06 23:11:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sexygrneyes</td>\n",
       "      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766711</th>\n",
       "      <td>0</td>\n",
       "      <td>2300048954</td>\n",
       "      <td>Tue Jun 23 13:40:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammydearr</td>\n",
       "      <td>@TiannaChaos i know  just family drama. its la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285055</th>\n",
       "      <td>0</td>\n",
       "      <td>1993474027</td>\n",
       "      <td>Mon Jun 01 10:26:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Lamb_Leanne</td>\n",
       "      <td>School email won't open  and I have geography ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705995</th>\n",
       "      <td>0</td>\n",
       "      <td>2256550904</td>\n",
       "      <td>Sat Jun 20 12:56:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>yogicerdito</td>\n",
       "      <td>upper airways problem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target         ids                          date      flag  \\\n",
       "541200       0  2200003196  Tue Jun 16 18:18:12 PDT 2009  NO_QUERY   \n",
       "750          0  1467998485  Mon Apr 06 23:11:14 PDT 2009  NO_QUERY   \n",
       "766711       0  2300048954  Tue Jun 23 13:40:11 PDT 2009  NO_QUERY   \n",
       "285055       0  1993474027  Mon Jun 01 10:26:07 PDT 2009  NO_QUERY   \n",
       "705995       0  2256550904  Sat Jun 20 12:56:51 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \n",
       "541200  LaLaLindsey0609             @chrishasboobs AHHH I HOPE YOUR OK!!!   \n",
       "750         sexygrneyes  @misstoriblack cool , i have no tweet apps  fo...  \n",
       "766711       sammydearr  @TiannaChaos i know  just family drama. its la...  \n",
       "285055      Lamb_Leanne  School email won't open  and I have geography ...  \n",
       "705995      yogicerdito                             upper airways problem   "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_text_preprocessing(content):\n",
    "\n",
    "    # Initialize lemmatizer\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Convert to lowercase and remove special characters\n",
    "    lemmatized_content = re.sub(r'http\\S+|www\\S+|https\\S+', '', content, flags=re.MULTILINE)  # Remove URLs\n",
    "    lemmatized_content = re.sub(r'@\\w+|\\#', '', lemmatized_content)  # Remove mentions and hashtag symbols\n",
    "    lemmatized_content = re.sub(r'[^a-zA-Z\\s]', '', lemmatized_content)  # Remove numbers and punctuation\n",
    "    lemmatized_content = lemmatized_content.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    words = word_tokenize(lemmatized_content)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Add custom stopwords that might not be useful for sentiment\n",
    "    custom_stopwords = {'rt', 'via'}\n",
    "    stop_words.update(custom_stopwords)\n",
    "    processed_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 2\n",
    "    ]\n",
    "    lemmatized_content = ' '.join(processed_words)\n",
    "    return lemmatized_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text preprocessing...\n"
     ]
    }
   ],
   "source": [
    " # Apply advanced text preprocessing\n",
    "    \n",
    "print(\"Applying text preprocessing...\")\n",
    "    \n",
    "data['lemmatized_content'] = data['text'].apply(advanced_text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_model(data, epochs=3, batch_size=32, learning_rate=2e-5):\n",
    "    \"\"\"\n",
    "    Train a BERT model for sentiment analysis\n",
    "    \n",
    "    Parameters:\n",
    "    data: pandas DataFrame with 'lemmatized_content' and 'target' columns\n",
    "    epochs: number of training epochs\n",
    "    batch_size: batch size for training\n",
    "    learning_rate: learning rate for optimization\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=2\n",
    "    ).to(device)\n",
    "\n",
    "    # Prepare data\n",
    "    X = data['lemmatized_content'].values\n",
    "    y = data['target'].values\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TwitterDataset(X_train, y_train, tokenizer)\n",
    "    val_dataset = TwitterDataset(X_val, y_val, tokenizer)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_true = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc='Training'):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=targets\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            train_preds.extend(preds)\n",
    "            train_true.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_accuracy = accuracy_score(train_true, train_preds)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=targets\n",
    "                )\n",
    "\n",
    "                val_losses.append(outputs.loss.item())\n",
    "                \n",
    "                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "                val_preds.extend(preds)\n",
    "                val_true.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        \n",
    "        print(f'\\nTraining Loss: {np.mean(train_losses):.4f}')\n",
    "        print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "        print(f'Validation Loss: {np.mean(val_losses):.4f}')\n",
    "        print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            print(\"\\nSaving best model...\")\n",
    "            torch.save(model.state_dict(), 'best_bert_model.pt')\n",
    "            \n",
    "        print('\\nClassification Report:')\n",
    "        print(classification_report(val_true, val_preds))\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device=None):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a single text input with CUDA optimization\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.eval()\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if device.type == 'cuda':\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    return \"Positive\" if prediction == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nikhi\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 750/750 [18:30<00:00,  1.48s/it]\n",
      "Validation: 100%|██████████| 188/188 [00:41<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.5130\n",
      "Training Accuracy: 0.7440\n",
      "Validation Loss: 0.4911\n",
      "Validation Accuracy: 0.7715\n",
      "\n",
      "Saving best model...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.76      3000\n",
      "           1       0.75      0.82      0.78      3000\n",
      "\n",
      "    accuracy                           0.77      6000\n",
      "   macro avg       0.77      0.77      0.77      6000\n",
      "weighted avg       0.77      0.77      0.77      6000\n",
      "\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 750/750 [18:28<00:00,  1.48s/it]\n",
      "Validation: 100%|██████████| 188/188 [00:41<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.4067\n",
      "Training Accuracy: 0.8170\n",
      "Validation Loss: 0.4825\n",
      "Validation Accuracy: 0.7775\n",
      "\n",
      "Saving best model...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.77      3000\n",
      "           1       0.77      0.80      0.78      3000\n",
      "\n",
      "    accuracy                           0.78      6000\n",
      "   macro avg       0.78      0.78      0.78      6000\n",
      "weighted avg       0.78      0.78      0.78      6000\n",
      "\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 750/750 [18:25<00:00,  1.47s/it]\n",
      "Validation: 100%|██████████| 188/188 [00:41<00:00,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.2952\n",
      "Training Accuracy: 0.8764\n",
      "Validation Loss: 0.6069\n",
      "Validation Accuracy: 0.7687\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.74      0.76      3000\n",
      "           1       0.76      0.80      0.77      3000\n",
      "\n",
      "    accuracy                           0.77      6000\n",
      "   macro avg       0.77      0.77      0.77      6000\n",
      "weighted avg       0.77      0.77      0.77      6000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = train_bert_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test text: This is a wonderful day! I'm really happy!\n",
      "Predicted sentiment: Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_7592\\3602477452.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "test_text = \"This is a wonderful day! I'm really happy!\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
    "print(f\"\\nTest text: {test_text}\")\n",
    "print(f\"Predicted sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Sentiment Analysis Project Report\n",
    "\n",
    "## Project Overview\n",
    "This project implements a sentiment analysis classifier using BERT (Bidirectional Encoder Representations from Transformers) on a Twitter dataset to predict tweet sentiment.\n",
    "\n",
    "## Model Architecture\n",
    "- **Base Model**: BERT-base-uncased\n",
    "- **Classification Type**: Binary Sentiment Classification\n",
    "- **Input Preprocessing**: Advanced text lemmatization and cleaning\n",
    "\n",
    "## Dataset Characteristics\n",
    "- **Total Samples**: 30,000 tweets\n",
    "- **Training Split**: 24,000 tweets\n",
    "- **Validation Split**: 6,000 tweets\n",
    "- **Class Distribution**: Balanced (50% positive, 50% negative)\n",
    "\n",
    "## Training Configuration\n",
    "- **Training Epochs**: 3\n",
    "- **Batch Size**: 32\n",
    "- **Learning Rate**: 2e-5\n",
    "- **Device**: CUDA (GPU-accelerated)\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "### Epoch-wise Performance\n",
    "| Epoch | Training Loss | Training Accuracy | Validation Loss | Validation Accuracy |\n",
    "|-------|--------------|-------------------|-----------------|---------------------|\n",
    "| 1     | 0.5130       | 0.7440            | 0.4911          | 0.7715              |\n",
    "| 2     | 0.4067       | 0.8170            | 0.4825          | 0.7775              |\n",
    "| 3     | 0.2952       | 0.8764            | 0.6069          | 0.7687              |\n",
    "\n",
    "### Final Model Performance\n",
    "| Metric       | Value |\n",
    "|--------------|-------|\n",
    "| Accuracy     | 0.7775 (77.75%) |\n",
    "| Precision (Negative) | 0.79 |\n",
    "| Precision (Positive) | 0.77 |\n",
    "| Recall (Negative)    | 0.76 |\n",
    "| Recall (Positive)    | 0.80 |\n",
    "| F1-Score (Negative)  | 0.77 |\n",
    "| F1-Score (Positive)  | 0.78 |\n",
    "\n",
    "## Key Observations\n",
    "1. **Model Learning**: \n",
    "   - Training accuracy increases with each epoch\n",
    "   - Validation accuracy remains relatively stable\n",
    "2. **Generalization**: Moderate overfitting observed\n",
    "3. **Performance Consistency**: Balanced performance across both sentiment classes\n",
    "\n",
    "## Preprocessing Techniques\n",
    "- URL removal\n",
    "- Mention and hashtag symbol removal\n",
    "- Lowercase conversion\n",
    "- Lemmatization\n",
    "- Stopword elimination\n",
    "- Custom stopword addition\n",
    "\n",
    "## Technical Stack\n",
    "- **Deep Learning Framework**: PyTorch\n",
    "- **Transformer Library**: Hugging Face Transformers\n",
    "- **Preprocessing**: NLTK\n",
    "- **Model**: BERT-base-uncased\n",
    "\n",
    "## Computational Resources\n",
    "- **Device**: CUDA-enabled GPU\n",
    "- **GPU**: Detected and utilized for training\n",
    "\n",
    "## Recommendations for Future Improvement\n",
    "1. Experiment with larger BERT variants (e.g., RoBERTa, BERT-large)\n",
    "2. Implement data augmentation techniques\n",
    "3. Use learning rate scheduling\n",
    "4. Explore ensemble methods\n",
    "5. Collect more diverse training data\n",
    "\n",
    "## Sample Prediction\n",
    "- **Test Text**: \"This is a wonderful day! I'm really happy!\"\n",
    "- **Predicted Sentiment**: Positive\n",
    "\n",
    "## Conclusion\n",
    "The BERT-based sentiment analysis model provides a robust solution for classifying tweet sentiments, achieving nearly 78% accuracy with sophisticated deep learning techniques.\n",
    "\n",
    "**Date of Analysis**: November 28, 2024\n",
    "**Dataset**: Twitter Sentiment Dataset\n",
    "**Total Processed Samples**: 30,000 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
