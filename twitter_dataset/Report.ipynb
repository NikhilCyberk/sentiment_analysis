{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Sentiment Analysis Model Performance Evaluation\n",
    "\n",
    "## 1. Detailed Model Performance Metrics\n",
    "\n",
    "### 1.1 Logistic Regression Model\n",
    "| Metric | Negative | Positive | Overall |\n",
    "|--------|----------|----------|---------|\n",
    "| Precision | 0.75 | 0.74 | 0.74 |\n",
    "| Recall | 0.73 | 0.76 | 0.74 |\n",
    "| F1-Score | 0.74 | 0.75 | 0.74 |\n",
    "| Training Accuracy | 80.94% |\n",
    "| Test Accuracy | 74.38% |\n",
    "\n",
    "### 1.2 Random Forest Model\n",
    "| Metric | Negative | Positive | Overall |\n",
    "|--------|----------|----------|---------|\n",
    "| Precision | 0.73 | 0.73 | 0.73 |\n",
    "| Recall | 0.72 | 0.73 | 0.73 |\n",
    "| F1-Score | 0.73 | 0.73 | 0.73 |\n",
    "| Training Accuracy | 98.40% |\n",
    "| Test Accuracy | 72.77% |\n",
    "\n",
    "### 1.3 LSTM Model\n",
    "| Metric | Negative | Positive | Overall |\n",
    "|--------|----------|----------|---------|\n",
    "| Precision | 0.00 | 0.50 | 0.25 |\n",
    "| Recall | 0.00 | 1.00 | 0.50 |\n",
    "| F1-Score | 0.00 | 0.67 | 0.33 |\n",
    "| Training Accuracy | 50.03% |\n",
    "| Test Accuracy | 50.03% |\n",
    "\n",
    "### 1.4 CNN-LSTM-GRU Hybrid Model\n",
    "| Metric | Negative | Positive | Overall |\n",
    "|--------|----------|----------|---------|\n",
    "| Precision | 0.72 | 0.74 | 0.73 |\n",
    "| Recall | 0.74 | 0.71 | 0.73 |\n",
    "| F1-Score | 0.73 | 0.72 | 0.73 |\n",
    "| Training Accuracy | 84.15% |\n",
    "| Test Accuracy | 72.82% |\n",
    "\n",
    "### 1.5 BERT Model\n",
    "| Metric | Negative | Positive | Overall |\n",
    "|--------|----------|----------|---------|\n",
    "| Precision | 0.83 | 0.84 | 0.83 |\n",
    "| Recall | 0.84 | 0.83 | 0.83 |\n",
    "| F1-Score | 0.83 | 0.83 | 0.83 |\n",
    "| Training Accuracy | 80.78% |\n",
    "| Test Accuracy | 83.00% |\n",
    "\n",
    "### 1.6 DistilBERT Model\n",
    "| Metric | Negative | Positive | Overall |\n",
    "|--------|----------|----------|---------|\n",
    "| Precision | 0.76 | 0.86 | 0.81 |\n",
    "| Recall | 0.87 | 0.73 | 0.80 |\n",
    "| F1-Score | 0.82 | 0.79 | 0.80 |\n",
    "| Training Accuracy | - |\n",
    "| Test Accuracy | 80.00% |\n",
    "\n",
    "### 1.7 CardiffNLP RoBERTa Model\n",
    "| Metric | Negative | Positive | Overall |\n",
    "|--------|----------|----------|---------|\n",
    "| Precision | 0.82 | 0.87 | 0.85 |\n",
    "| Recall | 0.88 | 0.81 | 0.85 |\n",
    "| F1-Score | 0.85 | 0.84 | 0.85 |\n",
    "| Test Accuracy | 85.00% |\n",
    "\n",
    "## 2. Comprehensive Model Comparison\n",
    "\n",
    "### 2.1 Accuracy Comparison\n",
    "| Model | Test Accuracy | Training Accuracy | Dataset Size |\n",
    "|-------|---------------|-------------------|--------------|\n",
    "| LSTM | 50.03% | 50.03% | 100,000 tweets |\n",
    "| Random Forest | 72.77% | 98.40% | 1,600,000 tweets |\n",
    "| CNN-LSTM-GRU Hybrid | 72.82% | 84.15% | 30,000 tweets |\n",
    "| Logistic Regression | 74.38% | 80.94% | 1,600,000 tweets |\n",
    "| DistilBERT | 80.00% | - | 4,000 tweets |\n",
    "| BERT | 83.00% | 80.78% | 30,000 tweets |\n",
    "| CardiffNLP RoBERTa | 85.00% | - | 50,000 tweets |\n",
    "\n",
    "### 2.2 Precision Comparison\n",
    "| Model | Negative Precision | Positive Precision | Overall Precision |\n",
    "|-------|--------------------|--------------------|-------------------|\n",
    "| LSTM | 0.00 | 0.50 | 0.25 |\n",
    "| Random Forest | 0.73 | 0.73 | 0.73 |\n",
    "| CNN-LSTM-GRU Hybrid | 0.72 | 0.74 | 0.73 |\n",
    "| Logistic Regression | 0.75 | 0.74 | 0.74 |\n",
    "| DistilBERT | 0.76 | 0.86 | 0.81 |\n",
    "| BERT | 0.83 | 0.84 | 0.83 |\n",
    "| CardiffNLP RoBERTa | 0.82 | 0.87 | 0.85 |\n",
    "\n",
    "### 2.3 Recall Comparison\n",
    "| Model | Negative Recall | Positive Recall | Overall Recall |\n",
    "|-------|-----------------|-----------------|----------------|\n",
    "| LSTM | 0.00 | 1.00 | 0.50 |\n",
    "| Random Forest | 0.72 | 0.73 | 0.73 |\n",
    "| CNN-LSTM-GRU Hybrid | 0.74 | 0.71 | 0.73 |\n",
    "| Logistic Regression | 0.73 | 0.76 | 0.74 |\n",
    "| DistilBERT | 0.87 | 0.73 | 0.80 |\n",
    "| BERT | 0.84 | 0.83 | 0.83 |\n",
    "| CardiffNLP RoBERTa | 0.88 | 0.81 | 0.85 |\n",
    "\n",
    "### 2.4 F1-Score Comparison\n",
    "| Model | Negative F1-Score | Positive F1-Score | Overall F1-Score |\n",
    "|-------|-------------------|-------------------|-------------------|\n",
    "| LSTM | 0.00 | 0.67 | 0.33 |\n",
    "| Random Forest | 0.73 | 0.73 | 0.73 |\n",
    "| CNN-LSTM-GRU Hybrid | 0.73 | 0.72 | 0.73 |\n",
    "| Logistic Regression | 0.74 | 0.75 | 0.74 |\n",
    "| DistilBERT | 0.82 | 0.79 | 0.80 |\n",
    "| BERT | 0.83 | 0.83 | 0.83 |\n",
    "| CardiffNLP RoBERTa | 0.85 | 0.84 | 0.85 |\n",
    "\n",
    "## 3. Training Efficiency Analysis\n",
    "\n",
    "### 3.1 Computational Resources\n",
    "| Model | Total Runtime | Samples/Second | Training Steps | Batch Size |\n",
    "|-------|---------------|----------------|----------------|------------|\n",
    "| BERT | 1,801.79 sec | 26.64 | 6,000 | 8 |\n",
    "| DistilBERT | 458.33 sec | 104.73 | - | 16 |\n",
    "| CardiffNLP RoBERTa | 19,039 sec | 6.299 | - | 16 |\n",
    "\n",
    "### 3.2 Model Configuration\n",
    "| Model | Embedding Dimension | Layers | Optimizer | Learning Rate |\n",
    "|-------|---------------------|--------|-----------|---------------|\n",
    "| LSTM | 128 | 2 LSTM (128, 64 units) | Adam | 0.0005 |\n",
    "| CNN-LSTM-GRU Hybrid | 200 | CNN, LSTM, GRU | Adam | - |\n",
    "| BERT | - | BERT-base-uncased | - | 3e-5 |\n",
    "| DistilBERT | - | Lightweight BERT | Adam | 2e-5 |\n",
    "| CardiffNLP RoBERTa | - | RoBERTa | - | 2e-5 |\n",
    "\n",
    "## 4. Key Observations and Recommendations\n",
    "\n",
    "### 4.1 Model Performance Ranking\n",
    "1. CardiffNLP RoBERTa (85.00% accuracy)\n",
    "2. BERT (83.00% accuracy)\n",
    "3. DistilBERT (80.00% accuracy)\n",
    "4. Logistic Regression (74.38% accuracy)\n",
    "5. CNN-LSTM-GRU Hybrid (72.82% accuracy)\n",
    "6. Random Forest (72.77% accuracy)\n",
    "7. LSTM (50.03% accuracy)\n",
    "\n",
    "### 4.2 Recommendations\n",
    "1. **Best Overall Performance**: CardiffNLP RoBERTa\n",
    "2. **Most Efficient**: DistilBERT\n",
    "3. **Best Traditional ML Model**: Logistic Regression\n",
    "4. **Avoid**: LSTM model due to poor performance\n",
    "\n",
    "### 4.3 Preprocessing Recommendations\n",
    "- Implement consistent text cleaning\n",
    "- Use lemmatization\n",
    "- Remove URLs and special characters\n",
    "- Balance class distribution\n",
    "- Use stratified sampling\n",
    "\n",
    "## Conclusion\n",
    "The landscape of sentiment analysis models shows significant variation in performance. Transformer-based models, particularly RoBERTa and BERT, demonstrate superior accuracy and balanced performance across positive and negative sentiments. The choice of model should consider computational resources, dataset characteristics, and specific project requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
