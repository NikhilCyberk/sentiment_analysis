{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure same preprocessing as previous script\n",
    "def advanced_text_preprocessing(content):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Convert to lowercase and remove special characters\n",
    "    lemmatized_content = re.sub(r'http\\S+|www\\S+|https\\S+', '', content, flags=re.MULTILINE)  # Remove URLs\n",
    "    lemmatized_content = re.sub(r'@\\w+|\\#', '', lemmatized_content)  # Remove mentions and hashtag symbols\n",
    "    lemmatized_content = re.sub(r'[^a-zA-Z\\s]', '', lemmatized_content)  # Remove numbers and punctuation\n",
    "    lemmatized_content = lemmatized_content.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    words = word_tokenize(lemmatized_content)\n",
    "\n",
    "    # Remove stopwords and lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stopwords = {'rt', 'via'}\n",
    "    stop_words.update(custom_stopwords)\n",
    "    processed_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 2\n",
    "    ]\n",
    "    lemmatized_content = ' '.join(processed_words)\n",
    "    return lemmatized_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load and preprocess the Twitter dataset\"\"\"\n",
    "    # column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "    data = pd.read_csv(\"Twitter_Data.csv\", encoding='ISO-8859-1')\n",
    "    data['category'] = data['category'].replace(-1, 2)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_learning_model(vocab_size, max_length):\n",
    "    \"\"\"Create a deep learning model with LSTM layers\"\"\"\n",
    "    model = Sequential([\n",
    "        # Embedding layer to convert words to dense vector representations\n",
    "        Embedding(vocab_size, 100, input_length=max_length),\n",
    "        \n",
    "        # LSTM layer with dropout for regularization\n",
    "        LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "        \n",
    "        # Additional dense layers with dropout\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_deep_learning_data(texts, max_words=5000, max_length=100):\n",
    "    \"\"\"Prepare text data for deep learning model\"\"\"\n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    # Convert text to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    # Pad sequences to ensure uniform length\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    \n",
    "    return padded_sequences, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_learning_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train and evaluate deep learning model\"\"\"\n",
    "    # Prepare data\n",
    "    max_words = 5000\n",
    "    max_length = 100\n",
    "    \n",
    "    # Prepare sequences\n",
    "    X_train_seq, tokenizer = prepare_deep_learning_data(X_train, max_words, max_length)\n",
    "    X_test_seq, _ = prepare_deep_learning_data(X_test, max_words, max_length)\n",
    "    \n",
    "    # Ensure binary targets\n",
    "    y_train = y_train.astype(float)\n",
    "    y_test = y_test.astype(float)\n",
    "    \n",
    "    # Create and compile model\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    model = create_deep_learning_model(vocab_size, max_length)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train, \n",
    "        epochs=10, \n",
    "        batch_size=32, \n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    train_pred = (model.predict(X_train_seq) > 0.5).astype(int).flatten()\n",
    "    test_pred = (model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "    \n",
    "    results = {\n",
    "        'Deep Learning Model': {\n",
    "            'train_accuracy': accuracy_score(y_train, train_pred),\n",
    "            'test_accuracy': accuracy_score(y_test, test_pred),\n",
    "            'classification_report': classification_report(y_test, test_pred)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "    \n",
    "print(\"Loading and preprocessing data...\")\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised âminimum government maxim...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised âminimum government maxim...       2.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "# apply on 30k data\n",
    "# data = data.sample(n=100000, random_state=42)\n",
    "\n",
    "data['lemmatized_content'] = data['clean_text'].astype(str).apply(advanced_text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "      <th>lemmatized_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised âminimum government maxim...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>modi promised minimum government maximum gover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>talk nonsense continue drama vote modi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>say vote modi welcome bjp told rahul main camp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>asking supporter prefix chowkidar name modi gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>answer among powerful world leader today trump...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category  \\\n",
       "0  when modi promised âminimum government maxim...       2.0   \n",
       "1  talk all the nonsense and continue all the dra...       0.0   \n",
       "2  what did just say vote for modi  welcome bjp t...       1.0   \n",
       "3  asking his supporters prefix chowkidar their n...       1.0   \n",
       "4  answer who among these the most powerful world...       1.0   \n",
       "\n",
       "                                  lemmatized_content  \n",
       "0  modi promised minimum government maximum gover...  \n",
       "1             talk nonsense continue drama vote modi  \n",
       "2  say vote modi welcome bjp told rahul main camp...  \n",
       "3  asking supporter prefix chowkidar name modi gr...  \n",
       "4  answer among powerful world leader today trump...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = data['lemmatized_content'].values\n",
    "y = data['category'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162969, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "      <th>lemmatized_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised âminimum government maxim...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>modi promised minimum government maximum gover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>talk nonsense continue drama vote modi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>say vote modi welcome bjp told rahul main camp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>asking supporter prefix chowkidar name modi gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>answer among powerful world leader today trump...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category  \\\n",
       "0  when modi promised âminimum government maxim...       2.0   \n",
       "1  talk all the nonsense and continue all the dra...       0.0   \n",
       "2  what did just say vote for modi  welcome bjp t...       1.0   \n",
       "3  asking his supporters prefix chowkidar their n...       1.0   \n",
       "4  answer who among these the most powerful world...       1.0   \n",
       "\n",
       "                                  lemmatized_content  \n",
       "0  modi promised minimum government maximum gover...  \n",
       "1             talk nonsense continue drama vote modi  \n",
       "2  say vote modi welcome bjp told rahul main camp...  \n",
       "3  asking supporter prefix chowkidar name modi gr...  \n",
       "4  answer among powerful world leader today trump...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training deep learning model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 64ms/step - accuracy: 0.4442 - loss: 0.3959 - val_accuracy: 0.4401 - val_loss: 0.3676\n",
      "Epoch 2/10\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 66ms/step - accuracy: 0.4453 - loss: 0.3746 - val_accuracy: 0.4401 - val_loss: 0.3708\n",
      "Epoch 3/10\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 67ms/step - accuracy: 0.4432 - loss: 0.3709 - val_accuracy: 0.4401 - val_loss: 0.3677\n",
      "Epoch 4/10\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 68ms/step - accuracy: 0.4434 - loss: 0.3767 - val_accuracy: 0.4401 - val_loss: 0.3679\n",
      "Epoch 5/10\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 66ms/step - accuracy: 0.4420 - loss: 0.3732 - val_accuracy: 0.4401 - val_loss: 0.3676\n",
      "Epoch 6/10\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 67ms/step - accuracy: 0.4437 - loss: 0.3701 - val_accuracy: 0.4401 - val_loss: 0.3677\n",
      "Epoch 7/10\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 67ms/step - accuracy: 0.4443 - loss: 0.3718 - val_accuracy: 0.4401 - val_loss: 0.3684\n",
      "Epoch 8/10\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 72ms/step - accuracy: 0.4417 - loss: 0.3736 - val_accuracy: 0.4401 - val_loss: 0.3676\n",
      "Epoch 9/10\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 69ms/step - accuracy: 0.4484 - loss: 0.3662 - val_accuracy: 0.4401 - val_loss: 0.3675\n",
      "Epoch 10/10\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 69ms/step - accuracy: 0.4444 - loss: 0.3671 - val_accuracy: 0.4401 - val_loss: 0.3676\n",
      "\u001b[1m4075/4075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 17ms/step\n",
      "\u001b[1m1019/1019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\nikhi\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\nikhi\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train deep learning model\n",
    "print(\"Training deep learning model...\")\n",
    "\n",
    "dl_results, dl_model, tokenizer = train_deep_learning_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deep Learning Model Results:\n",
      "Training Accuracy: 0.4433\n",
      "Test Accuracy: 0.4433\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     11042\n",
      "         1.0       0.44      1.00      0.61     14450\n",
      "         2.0       0.00      0.00      0.00      7102\n",
      "\n",
      "    accuracy                           0.44     32594\n",
      "   macro avg       0.15      0.33      0.20     32594\n",
      "weighted avg       0.20      0.44      0.27     32594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, metrics in dl_results.items():\n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"Training Accuracy: {metrics['train_accuracy']:.4f}\")\n",
    "        print(f\"Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(metrics['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model Performance Report\n",
    "\n",
    "## Model Overview\n",
    "- **Model Type**: LSTM Neural Network\n",
    "- **Dataset**: Twitter Sentiment Analysis\n",
    "- **Training Epochs**: 10\n",
    "- **Batch Size**: 32\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "### Accuracy\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Training Accuracy | 0.4433 |\n",
    "| Test Accuracy | 0.4433 |\n",
    "\n",
    "### Classification Metrics by Class\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|-----------|--------|----------|---------|\n",
    "| 0.0 | 0.00 | 0.00 | 0.00 | 11,042 |\n",
    "| 1.0 | 0.44 | 1.00 | 0.61 | 14,450 |\n",
    "| 2.0 | 0.00 | 0.00 | 0.00 | 7,102 |\n",
    "\n",
    "### Average Metrics\n",
    "\n",
    "| Average Type | Precision | Recall | F1-Score |\n",
    "|-------------|-----------|--------|----------|\n",
    "| Macro Avg | 0.15 | 0.33 | 0.20 |\n",
    "| Weighted Avg | 0.20 | 0.44 | 0.27 |\n",
    "\n",
    "## Key Observations\n",
    "- Significant class imbalance\n",
    "- Weak performance for classes 0.0 and 2.0\n",
    "- Strong bias towards predicting class 1.0\n",
    "- Overall low accuracy (44%)\n",
    "\n",
    "## Recommendations\n",
    "1. Address class imbalance:\n",
    "   - Oversampling minority classes\n",
    "   - Using class weights\n",
    "   - Applying SMOTE\n",
    "2. Investigate feature engineering\n",
    "3. Explore alternative model architectures\n",
    "4. Collect more balanced training data\n",
    "\n",
    "## Potential Improvements\n",
    "- Experiment with embedding dimensions\n",
    "- Try alternative neural network architectures\n",
    "- Implement advanced text preprocessing\n",
    "- Use transfer learning with pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
