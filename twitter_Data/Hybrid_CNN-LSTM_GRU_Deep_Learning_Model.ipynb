{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LDmJxJc0MnJv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: gensim in c:\\users\\nikhi\\appdata\\roaming\\python\\python312\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\nikhi\\appdata\\roaming\\python\\python312\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\nikhi\\appdata\\roaming\\python\\python312\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\nikhi\\appdata\\roaming\\python\\python312\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\nikhi\\appdata\\roaming\\python\\python312\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Z_6S9_wFMnJw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, Bidirectional, LSTM, GRU, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "npJtisWoMnJx"
   },
   "outputs": [],
   "source": [
    "def advanced_text_preprocessing(content):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Comprehensive text cleaning\n",
    "    content = re.sub(r'http\\S+|www\\S+|https\\S+', '', content, flags=re.MULTILINE)\n",
    "    content = re.sub(r'@\\w+', '', content)\n",
    "    content = re.sub(r'\\#', '', content)\n",
    "    content = re.sub(r'[^a-zA-Z\\s]', '', content)\n",
    "    content = content.lower()\n",
    "\n",
    "    # Advanced tokenization\n",
    "    words = word_tokenize(content)\n",
    "\n",
    "    # Enhanced stopwords and lemmatization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stopwords = {'rt', 'via', 'amp', 'u', 'ur'}\n",
    "    stop_words.update(custom_stopwords)\n",
    "\n",
    "    processed_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words\n",
    "        if word not in stop_words and len(word) > 1\n",
    "    ]\n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "viR9kfqYMnJx"
   },
   "outputs": [],
   "source": [
    "def create_hybrid_model(vocab_size, max_length, embedding_dim=200):\n",
    "    \"\"\"Create a hybrid CNN-LSTM-GRU model\"\"\"\n",
    "    # Input layer\n",
    "    input_layer = tf.keras.layers.Input(shape=(max_length,))\n",
    "\n",
    "    # Embedding layer\n",
    "    embedding = Embedding(\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        input_length=max_length,\n",
    "        trainable=True\n",
    "    )(input_layer)\n",
    "\n",
    "    # Spatial dropout\n",
    "    x = SpatialDropout1D(0.3)(embedding)\n",
    "\n",
    "    # Parallel processing branches\n",
    "    # CNN branch\n",
    "    cnn = Conv1D(128, 3, activation='relu')(x)\n",
    "    cnn = MaxPooling1D(3)(cnn)\n",
    "    cnn = Flatten()(cnn)\n",
    "\n",
    "    # LSTM branch\n",
    "    lstm = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    lstm = GlobalAveragePooling1D()(lstm)\n",
    "\n",
    "    # GRU branch\n",
    "    gru = Bidirectional(GRU(64))(x)\n",
    "\n",
    "    # Concatenate features\n",
    "    merged = concatenate([\n",
    "        cnn,\n",
    "        lstm,\n",
    "        gru\n",
    "    ])\n",
    "\n",
    "    # Dense layers\n",
    "    x = Dense(256, activation='relu')(merged)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    # Compile with advanced optimizer\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0003),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FviU2EjTMnJx"
   },
   "outputs": [],
   "source": [
    "def train_advanced_model(X_train, y_train, X_test, y_test):\n",
    "    # Prepare sequences\n",
    "    max_words = 15000\n",
    "    max_length = 200\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    # Pad sequences\n",
    "    X_train_pad = pad_sequences(\n",
    "        X_train_seq,\n",
    "        maxlen=max_length,\n",
    "        padding='post',\n",
    "        truncating='post'\n",
    "    )\n",
    "    X_test_pad = pad_sequences(\n",
    "        X_test_seq,\n",
    "        maxlen=max_length,\n",
    "        padding='post',\n",
    "        truncating='post'\n",
    "    )\n",
    "\n",
    "    # Compute class weights\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    # Create model\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    model = create_hybrid_model(vocab_size, max_length)\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=4,\n",
    "        min_lr=0.000001\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        epochs=30,\n",
    "        batch_size=128,\n",
    "        validation_split=0.2,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    train_pred = (model.predict(X_train_pad) > 0.5).astype(int).flatten()\n",
    "    test_pred = (model.predict(X_test_pad) > 0.5).astype(int).flatten()\n",
    "\n",
    "    # Detailed results\n",
    "    results = {\n",
    "        'Enhanced Model': {\n",
    "            'train_accuracy': accuracy_score(y_train, train_pred),\n",
    "            'test_accuracy': accuracy_score(y_test, test_pred),\n",
    "            'classification_report': classification_report(y_test, test_pred),\n",
    "            'confusion_matrix': confusion_matrix(y_test, test_pred)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return results, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vg1BuneZMnJy",
    "outputId": "e39a6489-75fa-4c8a-ec19-d2d4c08dea1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fD6Ki08M6vg",
    "outputId": "c37555c9-634a-4a6a-dd02-3abdc5361374"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFCWHTRsM95z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRSwGn5UMnJz",
    "outputId": "a7ecf0da-b038-45a1-997e-c4340c5c55dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "# column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "data = pd.read_csv(\"Twitter_Data.csv\", encoding='ISO-8859-1')\n",
    "# Remove NaN values\n",
    "data = data.dropna()\n",
    "# data['target'] = data['target'].replace(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Rd4ZgFHoNQQ8",
    "outputId": "acbeb556-b653-4d4b-81a3-1c6a2575c8ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised âminimum government maxim...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised âminimum government maxim...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXhZnCHpOonB",
    "outputId": "7bc619dd-f664-4ed3-b248-c51114f0a160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['clean_text', 'category'], dtype='object')\n",
      "clean_text     object\n",
      "category      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PJLcnx7zMnJz"
   },
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "\n",
    "# data = data.sample(n=30000, random_state=42)\n",
    "\n",
    "data['processed_text'] = data['clean_text'].astype(str).apply(advanced_text_preprocessing)\n",
    "# data['processed_text'] = data['clean_text'].apply(advanced_text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9ug5dZOeMnJz"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = data['processed_text'].values\n",
    "y = data['category'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-8uuTXEMnJ0",
    "outputId": "4a97172b-5203-4e7a-fa47-fcb8d27f053e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training advanced hybrid model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m884s\u001b[0m 1s/step - accuracy: 0.4207 - loss: -3185.7737 - val_accuracy: 0.5581 - val_loss: -176873.3125 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m876s\u001b[0m 1s/step - accuracy: 0.5490 - loss: -904624.6250 - val_accuracy: 0.5627 - val_loss: -6904888.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m898s\u001b[0m 1s/step - accuracy: 0.5442 - loss: -13721802.0000 - val_accuracy: 0.5347 - val_loss: -44280652.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m910s\u001b[0m 1s/step - accuracy: 0.5458 - loss: -67837736.0000 - val_accuracy: 0.5457 - val_loss: -153060224.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m914s\u001b[0m 1s/step - accuracy: 0.5393 - loss: -210227888.0000 - val_accuracy: 0.5582 - val_loss: -381109024.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m934s\u001b[0m 1s/step - accuracy: 0.5452 - loss: -496769760.0000 - val_accuracy: 0.5579 - val_loss: -795254784.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m936s\u001b[0m 1s/step - accuracy: 0.5420 - loss: -987168704.0000 - val_accuracy: 0.5537 - val_loss: -1449235968.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m942s\u001b[0m 1s/step - accuracy: 0.5400 - loss: -1751804800.0000 - val_accuracy: 0.5512 - val_loss: -2431939584.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m957s\u001b[0m 1s/step - accuracy: 0.5446 - loss: -2856062976.0000 - val_accuracy: 0.5416 - val_loss: -3835214336.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m967s\u001b[0m 1s/step - accuracy: 0.5406 - loss: -4475829760.0000 - val_accuracy: 0.5531 - val_loss: -5763359232.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m972s\u001b[0m 1s/step - accuracy: 0.5435 - loss: -6616163328.0000 - val_accuracy: 0.5477 - val_loss: -8306250752.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m977s\u001b[0m 1s/step - accuracy: 0.5405 - loss: -9463096320.0000 - val_accuracy: 0.5463 - val_loss: -11583839232.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m974s\u001b[0m 1s/step - accuracy: 0.5401 - loss: -13104785408.0000 - val_accuracy: 0.5393 - val_loss: -15742126080.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m973s\u001b[0m 1s/step - accuracy: 0.5374 - loss: -18151442432.0000 - val_accuracy: 0.5365 - val_loss: -20909500416.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m982s\u001b[0m 1s/step - accuracy: 0.5367 - loss: -23861903360.0000 - val_accuracy: 0.5467 - val_loss: -27487090688.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m990s\u001b[0m 1s/step - accuracy: 0.5371 - loss: -31213352960.0000 - val_accuracy: 0.5488 - val_loss: -35246026752.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m990s\u001b[0m 1s/step - accuracy: 0.5388 - loss: -39279951872.0000 - val_accuracy: 0.5449 - val_loss: -44603801600.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m990s\u001b[0m 1s/step - accuracy: 0.5407 - loss: -50391601152.0000 - val_accuracy: 0.5468 - val_loss: -55830188032.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m993s\u001b[0m 1s/step - accuracy: 0.5416 - loss: -62155206656.0000 - val_accuracy: 0.5403 - val_loss: -68935450624.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m992s\u001b[0m 1s/step - accuracy: 0.5398 - loss: -75754954752.0000 - val_accuracy: 0.5491 - val_loss: -84683350016.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m989s\u001b[0m 1s/step - accuracy: 0.5390 - loss: -94617772032.0000 - val_accuracy: 0.5443 - val_loss: -102526394368.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m979s\u001b[0m 1s/step - accuracy: 0.5411 - loss: -113918918656.0000 - val_accuracy: 0.5441 - val_loss: -123176370176.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m986s\u001b[0m 1s/step - accuracy: 0.5407 - loss: -130109521920.0000 - val_accuracy: 0.5436 - val_loss: -147076055040.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m990s\u001b[0m 1s/step - accuracy: 0.5393 - loss: -162326462464.0000 - val_accuracy: 0.5509 - val_loss: -174385922048.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1011s\u001b[0m 1s/step - accuracy: 0.5364 - loss: -193034534912.0000 - val_accuracy: 0.5509 - val_loss: -205191299072.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m993s\u001b[0m 1s/step - accuracy: 0.5370 - loss: -222107500544.0000 - val_accuracy: 0.5435 - val_loss: -239462367232.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1007s\u001b[0m 1s/step - accuracy: 0.5382 - loss: -262697385984.0000 - val_accuracy: 0.5495 - val_loss: -279170187264.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1013s\u001b[0m 1s/step - accuracy: 0.5419 - loss: -299269718016.0000 - val_accuracy: 0.5448 - val_loss: -322774859776.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1004s\u001b[0m 1s/step - accuracy: 0.5396 - loss: -352754630656.0000 - val_accuracy: 0.5495 - val_loss: -372753629184.0000 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m965s\u001b[0m 1s/step - accuracy: 0.5395 - loss: -410262208512.0000 - val_accuracy: 0.5469 - val_loss: -427640881152.0000 - learning_rate: 3.0000e-04\n",
      "\u001b[1m4075/4075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 37ms/step\n",
      "\u001b[1m1019/1019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\nikhi\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\nikhi\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"Training advanced hybrid model...\")\n",
    "results, model, tokenizer = train_advanced_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "OmxK-cY_MnJ0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Model Results:\n",
      "Training Accuracy: 0.5563\n",
      "Test Accuracy: 0.5484\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.00      0.00      0.00      7102\n",
      "         0.0       0.43      1.00      0.60     11042\n",
      "         1.0       0.98      0.48      0.64     14450\n",
      "\n",
      "    accuracy                           0.55     32594\n",
      "   macro avg       0.47      0.49      0.41     32594\n",
      "weighted avg       0.58      0.55      0.49     32594\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[    0  7004    98]\n",
      " [    0 11010    32]\n",
      " [    0  7586  6864]]\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Training Accuracy: {metrics['train_accuracy']:.4f}\")\n",
    "    print(f\"Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(metrics['classification_report'])\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "3wb6q-pHMnJ0"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "RI8GQvZMMnJ0"
   },
   "outputs": [],
   "source": [
    "filename = 'twitter_model_deeplearning_v3.pkl'\n",
    "pickle.dump(model_name, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "kHafP8ibMnJ0"
   },
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('twitter_model_deeplearning_v3.pkl', 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBT-QUGoMnJ0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Sb8wwkngMnJ1"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis is a test tweet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(test)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "test = 'this is a test tweet'\n",
    "prediction = loaded_model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Performance Report\n",
    "\n",
    "## Overview\n",
    "**Model Type:** Hybrid CNN-LSTM-GRU Neural Network for Text Classification\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "| Metric Category | Detailed Metrics | Value | Interpretation |\n",
    "|----------------|-----------------|-------|----------------|\n",
    "| **Model Accuracy** | Overall Accuracy | 54.84% | Moderate performance, slightly better than random guessing |\n",
    "| | Training Accuracy | 55.63% | Consistent with test accuracy, minimal overfitting |\n",
    "\n",
    "## Detailed Classification Performance\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support | \n",
    "|-------|-----------|--------|----------|---------|\n",
    "| Negative (-1.0) | 0.00 | 0.00 | 0.00 | 7,102 |\n",
    "| Neutral (0.0) | 0.43 | 1.00 | 0.60 | 11,042 |\n",
    "| Positive (1.0) | 0.98 | 0.48 | 0.64 | 14,450 |\n",
    "\n",
    "## Model Complexity Analysis\n",
    "\n",
    "| Component | Description | Complexity |\n",
    "|-----------|-------------|------------|\n",
    "| Architecture | Hybrid CNN-LSTM-GRU | High |\n",
    "| Embedding Dimension | 200 | Medium |\n",
    "| Max Sequence Length | 200 | Medium |\n",
    "| Vocabulary Size | 15,000 | Large |\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "1. **Class Imbalance**\n",
    "   - Significant performance variation across classes\n",
    "   - Positive class shows high precision (0.98)\n",
    "   - Negative class shows complete misclassification\n",
    "\n",
    "2. **Model Limitations**\n",
    "   - Struggles with identifying negative sentiments\n",
    "   - High recall for neutral class (1.00)\n",
    "   - Moderate performance for positive class\n",
    "\n",
    "3. **Potential Improvements**\n",
    "   - Address class imbalance\n",
    "   - Enhance preprocessing for negative sentiment detection\n",
    "   - Consider advanced sampling techniques\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. Use class weights or advanced sampling techniques\n",
    "2. Experiment with feature engineering\n",
    "3. Explore more sophisticated preprocessing\n",
    "4. Consider ensemble methods\n",
    "\n",
    "## Confusion Matrix Breakdown\n",
    "\n",
    "| Predicted \\ Actual | Negative | Neutral | Positive |\n",
    "|--------------------|----------|---------|----------|\n",
    "| Negative | 0 | 7,004 | 98 |\n",
    "| Neutral | 0 | 11,010 | 32 |\n",
    "| Positive | 0 | 7,586 | 6,864 |\n",
    "\n",
    "**Note:** The model shows a strong bias towards neutral classification, particularly misclassifying negative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
