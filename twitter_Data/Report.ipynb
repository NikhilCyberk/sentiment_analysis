{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Sentiment Analysis Model Performance Report\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report provides a holistic overview of multiple sentiment analysis models developed using various machine learning and deep learning techniques on Twitter sentiment data.\n",
    "\n",
    "## Dataset Characteristics\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|---------|\n",
    "| **Total Samples** | 162,969 |\n",
    "| **Class Distribution** | - Positive: 44.3% (72,249 samples) <br> - Negative: 33.9% (55,211 samples) <br> - Neutral: 21.8% (35,509 samples) |\n",
    "\n",
    "## Model Performance Comparison\n",
    "\n",
    "| Model | Overall Accuracy | Best Performing Class | Preprocessing Techniques | Key Strengths | Key Limitations |\n",
    "|-------|-----------------|----------------------|-------------------------|--------------|-----------------|\n",
    "| **BERT Base Uncased** | 95.36% | Positive Sentiment | - Lemmatization <br> - Stopword removal | High accuracy <br> Minimal overfitting | Computationally intensive |\n",
    "| **Logistic Regression** | 87.91% | Negative Sentiment | - URL removal <br> - Lemmatization <br> - TF-IDF Vectorization | Balanced performance <br> Less computational overhead | Moderate complexity <br> Limited by linear decision boundary |\n",
    "| **Random Forest** | 85.33% | Negative Sentiment | - Text preprocessing <br> - Unigram/bigram consideration | Handles non-linear relationships | Higher variance <br> Weaker neutral sentiment prediction |\n",
    "| **LSTM Neural Network** | 44.33% | Positive Sentiment | Basic text preprocessing | Deep learning capabilities | Poor performance <br> Significant class imbalance |\n",
    "| **CNN-LSTM-GRU Hybrid** | 54.84% | Positive Sentiment | Advanced text cleaning | Complex architecture | Moderate accuracy <br> Weak negative sentiment detection |\n",
    "| **Enhanced Deep Learning Model** | 52.25% | Positive Sentiment | - Early stopping <br> - Adaptive learning rate | Balanced class performance | Overfitting <br> Low generalization |\n",
    "| **DistilBERT Model 1** | 93% | Neutral Sentiment | - URL removal <br> - Mention removal | Strong neutral sentiment detection | Slightly weaker negative sentiment |\n",
    "| **DistilBERT Model 2** | 98% | Neutral Sentiment | - Advanced text cleaning <br> - Class balancing | Exceptional performance <br> Consistent across classes | Minimal limitations |\n",
    "| **RoBERTa-based Model** | 98% | Neutral Sentiment | - Comprehensive text cleaning <br> - Special character removal | Robust architecture <br> High precision | Requires significant computational resources |\n",
    "\n",
    "## Detailed Performance Metrics\n",
    "\n",
    "### Precision, Recall, and F1-Score Comparison\n",
    "\n",
    "| Model | Negative Sentiment | Neutral Sentiment | Positive Sentiment |\n",
    "|-------|-------------------|-------------------|---------------------|\n",
    "| **BERT Base Uncased** | P: 0.92, R: 0.94, F1: 0.93 | - | P: 0.97, R: 0.96, F1: 0.96 |\n",
    "| **Logistic Regression** | P: 0.88, R: 0.93, F1: 0.90 | P: 0.84, R: 0.78, F1: 0.81 | P: 0.90, R: 0.89, F1: 0.89 |\n",
    "| **Random Forest** | P: 0.85, R: 0.94, F1: 0.89 | P: 0.85, R: 0.67, F1: 0.75 | P: 0.86, R: 0.88, F1: 0.87 |\n",
    "| **LSTM Neural Network** | P: 0.00, R: 0.00, F1: 0.00 | P: 0.43, R: 1.00, F1: 0.60 | P: 0.98, R: 0.48, F1: 0.64 |\n",
    "| **CNN-LSTM-GRU Hybrid** | P: 0.00, R: 0.00, F1: 0.00 | P: 0.43, R: 1.00, F1: 0.60 | P: 0.98, R: 0.48, F1: 0.64 |\n",
    "| **Enhanced Deep Learning Model** | P: 0.53, R: 0.47, F1: 0.49 | - | P: 0.52, R: 0.58, F1: 0.55 |\n",
    "| **DistilBERT Model 1** | P: 0.91, R: 0.83, F1: 0.87 | P: 0.97, R: 0.96, F1: 0.96 | P: 0.91, R: 0.96, F1: 0.94 |\n",
    "| **DistilBERT Model 2** | P: 0.96, R: 0.98, F1: 0.97 | P: 0.99, R: 0.98, F1: 0.98 | P: 0.98, R: 0.97, F1: 0.97 |\n",
    "| **RoBERTa-based Model** | P: 0.97, R: 0.98, F1: 0.97 | P: 0.99, R: 0.98, F1: 0.98 | P: 0.98, R: 0.98, F1: 0.98 |\n",
    "\n",
    "## Key Findings and Recommendations\n",
    "\n",
    "1. **Model Performance Hierarchy**\n",
    "   - Top Performers: RoBERTa-based Model, DistilBERT Model 2 (98% accuracy)\n",
    "   - Strong Performers: BERT Base Uncased, Logistic Regression, Random Forest\n",
    "   - Limited Performance: LSTM, CNN-LSTM-GRU Hybrid\n",
    "\n",
    "2. **Preprocessing Strategies**\n",
    "   - Most effective techniques:\n",
    "     * URL and mention removal\n",
    "     * Special character elimination\n",
    "     * Lemmatization\n",
    "     * Stopword removal\n",
    "     * Class balancing\n",
    "\n",
    "3. **Model Selection Criteria**\n",
    "   - Computational Resources\n",
    "   - Accuracy Requirements\n",
    "   - Computational Efficiency\n",
    "   - Domain-Specific Needs\n",
    "\n",
    "## Recommended Model Selection\n",
    "\n",
    "| Use Case | Recommended Model |\n",
    "|----------|-------------------|\n",
    "| High-Performance Production | RoBERTa-based Model or DistilBERT Model 2 |\n",
    "| Resource-Constrained Environments | Logistic Regression or Random Forest |\n",
    "| Exploratory Analysis | BERT Base Uncased |\n",
    "\n",
    "## Future Research Directions\n",
    "\n",
    "1. Develop more sophisticated ensemble methods\n",
    "2. Explore domain-specific fine-tuning\n",
    "3. Investigate advanced class balancing techniques\n",
    "4. Reduce computational complexity of deep learning models\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The sentiment analysis landscape shows significant variation in model performance. While transformer-based models (RoBERTa and DistilBERT) demonstrate exceptional accuracy, traditional machine learning approaches remain viable alternatives depending on specific use cases and resource constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
